{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Asesoftware_logo.png\" width=\"200\" height=\"100\">\n",
    "<center>\n",
    "    <h1>Análisis Postmortem</h1>\n",
    "    <h3>Autores: <i>Laura Benavides, Rodolfo de la Rosa, Álvaro Valbuena</i></h3>\n",
    "    <h4><i>lbenavides@asesoftware.com, rdelarosap@asesoftware.com, avalbuena@asesoftware.com</i></h4>\n",
    "    <h3>Área de Innovación</h3>\n",
    "    <br>\n",
    "</center>\n",
    "\n",
    "\n",
    "El procesamiento de lenguaje natural (o NLP por sus siglas en inglés) es una rama de conocimiento de la Inteligencia Artificial y la lingüística que, esencialmente, tiene como objetivo que los computadores \"comprendan\" el lenguaje natural para así poder desarrollar tareas útiles.\n",
    "\n",
    "NLP involucra el análisis de:\n",
    "\n",
    "1. **Sintaxis**: identificar el rol sintáctico de cada palabra en la oración. E.g. dulces es objeto directo en: Yo compraré dulces.\n",
    "\n",
    "2. **Semántica**: determinar el significado de las palabras o frases. E.g. \"El ingeniero fue al cliente\". El cliente se refiere a un lugar.\n",
    "\n",
    "3. **Pragmática**: establecer como el contexto comunicativo afecta el significado. E.g. \"El tomó vino en la fiesta. Era rojo\". Rojo se refiere al vino.\n",
    "\n",
    "Los dos desafíos más comunes en NLP son: Ambigüedad y combinar contexto con conocimiento previo.\n",
    "\n",
    "Para el análisis de los documentos post-mortem se van a seguir el típico pipeline en NLP:\n",
    "\n",
    "1. *Tokenization*\n",
    "2. *Lemmatization or Stemming*\n",
    "3. *Sentence boundary detection*\n",
    "4. *POS tagging*\n",
    "5. *Parsing*\n",
    "6. *NER*\n",
    "7. *Coreference resolution*\n",
    "8. *Infomation Extraction*\n",
    "\n",
    "Durante la implementación de este proyecto se verán algunos problemas que no permitieron completar todos los pasos anteriores y que hicieron tomar otras aproximaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descargar paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge spacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda xlrd -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge textacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda nltk -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *Tokenization*\n",
    "\n",
    "El primer paso en el procesamiento de lenguaje natural es poder separar una sequencia de caracteres en una sequencia de tokens. Un token se define como una secuencia de caracteres con significado (normalmente son palabras, sin embargo pueden ser n-grams). A este proceso se le llama **_tokenization_**.\n",
    "\n",
    "Un ejemplo es usar el espacio para separar los tokens: \n",
    "\n",
    "### La inteligencia es la habilidad de adaptarse a los cambios.\n",
    "\n",
    "Se convierte:\n",
    "\n",
    "### | La | inteligencia | es | la | habilidad | de | adaptarse | a | los | cambios. |\n",
    "\n",
    "Las expresiones regulares son usadas para realizar este proceso. A continuación hay dos ejemplos de *tokenization* por medio de dos expresiones regulares diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "frase = 'La inteligencia es*la habilidad de adaptarse a los cambios...'\n",
    "\n",
    "# Tokenizador por expresiones regulares con espacios en blanco\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenizador por expresiones regulares para tokenizar texto de acuerdo al Penn Treebank.\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "print(regexp_tokenizer.tokenize(frase))\n",
    "print(treebank_tokenizer.tokenize(frase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. _Lemmatization or Stemming_\n",
    "\n",
    "El objetivo de este paso es normalizar o cannonizar los tokens a una forma reducida. Para esto, hay dos técnicas diferentes:\n",
    "\n",
    "- **Stemming:**\n",
    "\n",
    "En _stemming_ , o derivación regresiva, lo que se busca es obtener la raíz o *stem* de la palabra, quitando afijos mediante un algoritmo. Muchas veces el resultado no son palabras reales: computadora y computación tendrían como *stem* comput.\n",
    "\n",
    "En español, el algoritmo que mejor funciona es el Snowball Stemmer: https://snowballstem.org/algorithms/spanish/stemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming con NLTK\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def normalize(tokens):\n",
    "    normalized_tokens = list()\n",
    "    for token in tokens:\n",
    "        # Primero se pone todo el texto en minúscula\n",
    "        normalized = token.lower()\n",
    "        # Se llama el stem del NLTK\n",
    "        normalized = stemmer.stem(normalized)\n",
    "        normalized_tokens.append(normalized)\n",
    "    return normalized_tokens\n",
    "\n",
    "\n",
    "frase_normalizada1 = normalize(regexp_tokenizer.tokenize(frase)) \n",
    "\n",
    "print('Frase:',frase)\n",
    "print('Stems:',frase_normalizada1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Lemmatization:**\n",
    "\n",
    "Es el proceso de agrupar las diferentes formas de una palabra a una forma base o *lemma*. Este proceso usa  el POS tagging de las palabras, el contexto de la frase y hasta el contexto del corpus para generar el lemma de una palabra. \n",
    "\n",
    "Este proceso busca retornar por cada palabra una forma base tal cual se encontraría en un diccionario como Wordnet.\n",
    "\n",
    "Es un proceso más lingüíticamente adecuado que el *stemming* pero depende de la precisión del contexto y del diccionario. Este proceso es más lento y más costoso que el *stemming*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Importar vocabulario de Spacy, removiendo del pipeline el NER\n",
    "nlp = spacy.load('es_core_news_md', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word = []\n",
    "lemma = []\n",
    "punct = []\n",
    "space = []\n",
    "shape = []\n",
    "\n",
    "text_nlp=nlp(frase)\n",
    "for token in text_nlp:\n",
    "    word.append(token.text)\n",
    "    lemma.append(token.lemma_)\n",
    "    punct.append(token.is_punct)\n",
    "    space.append(token.is_space)\n",
    "    shape.append(token.shape_)\n",
    "\n",
    "    \n",
    "pd_lemmas = pd.DataFrame({'Word':word, 'Lemma':lemma, 'Punct':punct, 'Space':space, 'Shape':shape})\n",
    "# Muestra la palabra, el lemma, si es puntuación, si es espacio, y  la forma de la palabra\n",
    "pd_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence boundary detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part-Of-Speech (POS) tagging\n",
    "\n",
    "Consiste en asignarle (o etiquetar) a cada una de las palabras de un texto de su categoría gramatical (Nombre, pronombre, adjetivo, verbo...) de acuerdo a su contexto y su significado.\n",
    "\n",
    "Los dos POS-tagger más conocidos para español son:\n",
    "\n",
    "1. StanfordPOSTagger (NLTK)\n",
    "2. SpaCy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**<font color=red>NOTA:</font>** Para que el POS-tagger de Stanford funcione correctamente es necesario descargar los modelos completos de https://nlp.stanford.edu/software/tagger.shtml#Download y descomprimirlos en la ruta de las variables `tagger`y `jar` de la siguiente celda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging de Stanford\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "tagger = \"sources/models/spanish.tagger\"\n",
    "jar = \"sources/stanford-postagger.jar\"\n",
    "\n",
    "tagger = StanfordPOSTagger(tagger,jar)\n",
    "tags = tagger.tag(regexp_tokenizer.tokenize(frase.lower()))\n",
    "for tag in tags:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging de Spacy\n",
    "word = []\n",
    "lemma = []\n",
    "shape = []\n",
    "pos = []\n",
    "istop = []\n",
    "\n",
    "for token in text_nlp:\n",
    "    word.append(token.text)\n",
    "    lemma.append(token.lemma_)\n",
    "    shape.append(token.shape_)\n",
    "    pos.append(token.pos_)\n",
    "    istop.append(token.is_stop)\n",
    "    \n",
    "pd_pos = pd.DataFrame({'Word':word, 'Lemma':lemma, 'Shape':shape, 'POS':pos, 'is_stop':istop})\n",
    "pd_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 5. Parsing\n",
    "\n",
    "Existen varios tipos de parsing: *constituency, dependency y semantic.*\n",
    "\n",
    "Para este proyecto se decidió solo concentrarse en el *dependency parsing*.\n",
    "\n",
    "Este proceso analiza la estructura gramatical de una frase, estableciendo relaciones entre palabras \"origen\" y palabras que modifican esas palabra origen (o dependientes). \n",
    "\n",
    "Estas relaciones gramaticales son representadas como arcos entre las palabras de la oración. La dirección de los arcos indica la relación _desde_ la palabra origen _hacia_ la palabra dependiente.\n",
    "\n",
    "Esto genera un grafo dirigido por cada oración.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(text_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "Es el proceso de encontrar y clasificar nombres/entidades en el texto. Las clasificaciones varían según la herramienta que se use. Ejemplos de categorias pueden ser:\n",
    "\n",
    "1. Personas\n",
    "2. Locaciones\n",
    "3. Organizaciones\n",
    "4. Miscelánea\n",
    "\n",
    "Este proceso corresponde a usar un clasificador ya pre-entrenado con las palabras del texto que se está analizando. Estos clasificadores suelen ser *sequence-labeling models*. Los métodos más sofisticados usan RNN (redes neuronales recurrentes) bidireccionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Coreference Resolution\n",
    "\n",
    "Es el proceso mediante el cuál se encuentran todas las expresiones que se refieren a la misma entidad en el texto.\n",
    "\n",
    "e.g. \"**Obama** visitó la ciudad. **El presidente** habló sobre el cambio climático. **Él** mencionó los problemas ambientales que vendrán en los próximos años.\"\n",
    "\n",
    "Las palabras en negrilla se refieren a la misma entidad: Obama.\n",
    "\n",
    "Para implementar este paso, se usan modelos supervisados de machine learning o en algunos casos heurísticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Information Extraction\n",
    "\n",
    "Una de las tareas típicas en IE corresponde a identificar entidades, encontrar relaciones entre estas entidades y encontrar eventos. Las relaciones se pueden presentar en formas como: contiene, es, regula, causa, interactua, está compuesto...\n",
    "\n",
    "Toda la información recolectada hasta este paso es usada para transformar el texto escrito en lenguaje natural en información estructurada como por ejemplo una base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "<h1><center>NLP con archivos post-mortem</center></h1>\n",
    "\n",
    "En Asesoftware existen archivos con la información post-mortem de los proyectos, estos son los documentos de cierre de proyecto. En este documento, existe una sección donde se agrupan las lecciones aprendidas en el desarrollo del proyecto.\n",
    "\n",
    "Esta información está agrupada en un archivo excel donde se encuentra la siguiente información por proyecto: tipo de proyecto, contexto, lección aprendida, entre otras caracteristicas.\n",
    "\n",
    "El siguiente script obtiene en un dataframe de Pandas la información de contexto y lección aprendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para concatenar dos celdas.\n",
    "def concat_text(pdSeries):\n",
    "    pdSeries = pdSeries.str.rstrip('.')\n",
    "    return pdSeries.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo con las lecciones aprendidas\n",
    "nombre_archivo = \"data/REPOSITORIO_LECCIONES APRENDIDAS.xlsx\"\n",
    "\n",
    "# Lectura del archivo\n",
    "data = pd.read_excel(nombre_archivo, encoding='latin-1', keep_default_na= False, na_values=[\"\"])\n",
    "\n",
    "# Se concatena el contenido de las columnas \"CONTEXTO\"  y \"LECCIONES APRENDIDAS\"\n",
    "data[\"CONTEX_LECC\"] = [concat_text(i[1]) for i in data[['CONTEXTO', 'LECCIONES APRENDIDAS']].iterrows()]\n",
    "\n",
    "# Se quitan espacios y enter de la columna CONTEXT_LECC\n",
    "data.CONTEX_LECC = data.CONTEX_LECC.str.replace('\\n', ' ').replace('\\s+', ' ')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la columna `CONTEX_LECC` están concatenadas las oraciones de contexto y lecciones, y `cont_lecc` (en la siguiente celda) es el texto que se va a analizar con el pipeline de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_lecc = concat_text(data[\"CONTEX_LECC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Spacy, el modelo entrenado en español incluye en su _pipeline:_\n",
    "\n",
    "- Tokenizer\n",
    "- Lemmatizer\n",
    "- POS-tagger\n",
    "- NER\n",
    "\n",
    "Sin embargo, por los resultados obtenidos en pruebas concluimos que el NER de Spacy necesita ser mejorado para poder usarlo con confianza en este proceso de extracción de información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda se extraen los lemmas, el POS-tagging y el dependency parsing de la columna `CONTEX_LECC`, junto con una lista de _dependientes_ (children) de la palabra en el árbol del dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas correspondientes a cada componente\n",
    "word = []\n",
    "lemma = []\n",
    "shape = []\n",
    "pos = []\n",
    "istop = []\n",
    "dep = []\n",
    "head = []\n",
    "children = []\n",
    "\n",
    "nlp_text=nlp(cont_lecc)\n",
    "\n",
    "# Recorer la lista de tokens del texto y obtener su información respecto a lemma, forma, POStagging, si es stopword, \n",
    "# dependency parsing, raiz y ramas\n",
    "for token in nlp_text:\n",
    "    word.append(token.text)\n",
    "    lemma.append(token.lemma_)\n",
    "    shape.append(token.shape_)\n",
    "    pos.append(token.pos_)\n",
    "    istop.append(token.is_stop)\n",
    "    dep.append(token.dep_)\n",
    "    head.append(token.head.text)\n",
    "    children.append([child for child in token.children])\n",
    "\n",
    "    \n",
    "results = pd.DataFrame({'Word':word, 'Lemma':lemma, 'POS':pos, 'DEP':dep, 'head':head,\n",
    "                             'children':children, 'Shape':shape, 'is_stop':istop})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo, una vez obtenidas las categorías gramaticales (POS) y las dependencias es extraer entidades y sus relaciones.\n",
    "\n",
    "Esto se puede lograr mediante:\n",
    "* Reglas\n",
    "* Modelos de machine learning no supervisados (clustering o neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer reglas, se analizaron las gráficas de dependency parsing y se probó con las siguientes relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "\n",
    "for possible_subject in nlp_text:\n",
    "    # si el POStagging del HEAD de la palabra es VERB (verbo) y su dependency parsing es nsubj (sujeto nominal)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.dep_ == 'nsubj' :\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # si las ramas son nmod (modificador nominal) y no es espacio\n",
    "             if child.dep_ in ('nmod') and child.pos_ != 'SPACE': \n",
    "                children.append(child)\n",
    "            \n",
    "        if children:\n",
    "            info.append((possible_subject.head.lemma_.lower(),possible_subject.lemma_.lower(),children))\n",
    "result = pd.DataFrame(info, columns = ['Head' , 'Word', 'Children'])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    # Si el POStagging del HEAD de la palabra es VERB y el POStagging de la palabra es un sustantivo (PROPN y NOUN) y\n",
    "    # su dependency parsing es sujeto nominal (nsubj)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN') and possible_subject.dep_=='nsubj':\n",
    "        info.append((possible_subject.head,possible_subject,possible_subject.lemma_))\n",
    "        \n",
    "result_subj = pd.DataFrame(info, columns = ['Head','Word', 'Lemma'])\n",
    "result_subj.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    # Si el POStagging del HEAD de la palabra es VERB y el POS de la palabra es un sustantivo (PROPN y NOUN) y\n",
    "    # su dependency parsing es sujeto nominal (nsubj)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN') and possible_subject.dep_=='nsubj':\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # Solo agregar si no es identificado como espacio\n",
    "            if child.pos_ != 'SPACE':\n",
    "                children.append(child)\n",
    "            \n",
    "        if children:\n",
    "            info.append((possible_subject.head.lemma_,possible_subject,possible_subject.lemma_.lower(),children))\n",
    "            \n",
    "result_subj1 = pd.DataFrame(info, columns = ['Head' , 'Word', 'Lemma', 'Children'])\n",
    "result_subj1.head(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    # Si el POStagging de la palabra es VERB\n",
    "    if possible_subject.pos_ == 'VERB' :\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # si las ramas son nsubj (sujeto nominal) y no es espacio\n",
    "            if child.dep_=='nsubj' and child.pos_ != 'SPACE':\n",
    "                children.append(child)\n",
    "            \n",
    "        if children:\n",
    "            info.append((possible_subject.head.lemma_,possible_subject,children))\n",
    "            \n",
    "result_subj2 = pd.DataFrame(info, columns = ['Head' , 'Word', 'Children'])\n",
    "result_subj2.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconocimiento de entidades mediante reglas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import textacy\n",
    "from collections import defaultdict\n",
    "\n",
    "###\n",
    "# Patrón para extraer información de un texto basado en reglas (utilizando expresiones regulares) utilizando los resultados del postagging.\n",
    "###\n",
    "\n",
    "# se crea la siguiente regla:\n",
    "patron = r'<PROPN>+ (<PUNCT|CCONJ> <PUNCT|CCONJ>? <PROPN>+)*'\n",
    "\n",
    "# la regla se utiliza con la libreria textacy para encontrar las entidades de las leciones aprendidas.\n",
    "param = []\n",
    "i = 0\n",
    "while i < len(data[\"CONTEX_LECC\"]):\n",
    "    lists_ = []\n",
    "    sent = nlp(data[\"CONTEX_LECC\"].iloc[i])\n",
    "    doc = textacy.make_spacy_doc(sent, lang='es_core_news_md')\n",
    "    lists_ = textacy.extract.pos_regex_matches(doc, patron)\n",
    "    for item in lists_:\n",
    "        if len(item) != 0:\n",
    "            param.append(item.text.lower())\n",
    "    i +=1\n",
    "\n",
    "\n",
    "# Resultados obtenidos\n",
    "j=0\n",
    "aux = defaultdict(list)\n",
    "for index, item in enumerate(param):\n",
    "    aux[item].append(index)\n",
    "\n",
    "result = {item: len(indexs) for item, indexs in aux.items() if len(indexs) >= 1}\n",
    "entity_list = list(result.keys())\n",
    "entity_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para visualizar contenido de 'Entidades'\n",
    "entity_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que con esta regla se obtienen algunas entidades que estan referenciadas en el texto de forma diferente.\n",
    "\n",
    "Por ejemplo: Davivienda, Banco Davivienda, están haciendo referencia a la misma entidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinación de la regla de entidades con la regla de relaciones en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "\n",
    "for possible_subject in nlp_text:\n",
    "    \n",
    "    # Si la palabra se encuentra en la lista de entidades\n",
    "    if str(possible_subject).lower() in entity_list:\n",
    "        \n",
    "        # si el POStagging del HEAD de la palabra es VERB y su dependency parsing es nsubj (sujeto nominal)\n",
    "        if possible_subject.head.pos_ == 'VERB' and possible_subject.dep_ == 'nsubj' :\n",
    "            \n",
    "            for child in possible_subject.children:\n",
    "                # Si la palabra de la rama se encuentra en la lista de entidades            \n",
    "                if str(child).lower() in entity_list:\n",
    "                    \n",
    "                    # si las ramas son nmod (modificador nominal) y no es espacio\n",
    "                     if child.dep_ in ('nmod') and child.pos_ != 'SPACE':\n",
    "                            info.append((possible_subject.text.lower(),possible_subject.head.lemma_.lower(),child.text.lower()))\n",
    "\n",
    "result_ent = pd.DataFrame(info, columns = ['Word', 'Head', 'Children'])\n",
    "result_ent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    # Si la palabra se encuentra en la lista de entidades\n",
    "    if str(possible_subject).lower() in entity_list:\n",
    "        # Si el POStagging del HEAD de la palabra es VERB y el POS de la palabra es un sustantivo (PROPN y NOUN)\n",
    "        if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN'):\n",
    "\n",
    "            for child in possible_subject.children:\n",
    "                \n",
    "                # Si la palabra de la rama se encuentra en la lista de entidades\n",
    "                if str(child).lower() in entity_list:\n",
    "                    \n",
    "                    # Solo agregar si no es identificado como espacio\n",
    "                    if child.pos_ != 'SPACE':\n",
    "                        info.append((possible_subject.text.lower(),possible_subject.head.lemma_.lower(),child.text.lower()))\n",
    "\n",
    "result_ent1 = pd.DataFrame(info, columns = ['Word', 'Head', 'Children'])\n",
    "result_ent1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Palabras relacionadas con 'Asesoftware'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = 'asesoftware'\n",
    "asw_1 = result_ent[(result_ent.Word.str.contains(search)) | (result_ent.Children.str.contains(search))]\n",
    "asw_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asw_2= result_ent1[(result_ent1.Word.str.contains(search))| (result_ent1.Children.str.contains(search))]\n",
    "asw_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De las lecciones aprendidad al ejecutar las dierentes reglas se extrae información relevenate, por ejemplo de Asesoftware se dice lo siguiente:\n",
    "- Asesoftware guía y realiza procesos de prueba.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones:\n",
    "\n",
    "Durante el análisis de resultados, se observó que habían entidades \"ambiguas\", como _cliente_ o _proyecto_ y se consideró importante saber, por ejemplo, de qué cliente o tipo de proyecto se refería esta entidad.\n",
    "\n",
    "Ya que dentro del archivo de Lecciones aprendidas está esta información concreta (las columnas 'CLIENTE' y 'TIPO PROYECTO'), se planteó reemplazar las palabras por el contenido de esas columnas. Sin embargo, esta modificación afecta el POStagging y el dependency parsing, por lo que se decidió dejar de lado esta aproximación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
