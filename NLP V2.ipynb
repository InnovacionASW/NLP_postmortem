{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajar con archivos post-mortem\n",
    "\n",
    "En **<font color='0066FB'>Asesoftware</font>** existen archivos con la información post-mortem de los proyectos: los documentos de cierre de proyecto. En este documento, en la sección de lecciones aprendidas, se consignan los aprendizajes del proyecto.\n",
    "\n",
    "La información de cada lección aprendida, contexto, proyecto, etc. se encuentra en un archivo que representa la base de datos de lecciones aprendidas aprobadas por la organización. El siguiente script obtiene en un dataframe de Pandas la información de contexto y lección aprendida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# Análisis Post-Mortem\n",
    "\n",
    "El procesamiento de lenguaje natural (o NLP por sus siglas en inglés) es una rama de conocimiento de la Inteligencia Artificial que, esencialmente, pretende conseguir que una máquina comprenda lo que expresa una persona mediante el uso de una lengua natural.\n",
    "\n",
    "Para el análisis de los documentos post-mortem se van a seguir los siguientes pasos:\n",
    "\n",
    "1. Separación de palabras ( _tokenize_ )\n",
    "2. Limpieza ( _Stem - Lemma_ )\n",
    "3. _Part-Of-Speech tagging_\n",
    "4. _Dependency Parsing_\n",
    "5. Reconocimiento de entidades ( _Named Entity Recognition_ )\n",
    "\n",
    "Esto para poder llegar a encontrar **relaciones binarias** entre las entidades halladas en el punto 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descargar paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge spacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda pandas -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar vocabulario de Spacy, removiendo del pipeline el NER\n",
    "nlp = spacy.load('es_core_news_md', disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text(pdSeries):\n",
    "    pdSeries = pdSeries.str.rstrip('.')\n",
    "    return pdSeries.str.cat(sep='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_archivo = '../input/REPOSITORIO_LECCIONES APRENDIDAS.xlsx'\n",
    "\n",
    "data = pd.read_excel(nombre_archivo, encoding='latin-1', keep_default_na= False, na_values=[\"\"])\n",
    "data[\"CONTEX_LECC\"] = [concat_text(i[1]) for i in data[['CONTEXTO', 'LECCIONES APRENDIDAS']].iterrows()]\n",
    "data.CONTEX_LECC = data.CONTEX_LECC.str.replace('\\n', ' ').replace('\\s+', ' ')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la columna `CONTEX_LECC` están concatenadas las oraciones de contexto y lecciones, y `cont_lecc` (en la siguiente celda) es el texto que se va a analizar con el pipeline de NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_lecc = concat_text(data[\"CONTEX_LECC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Spacy, el modelo entrenado en español incluye en su _pipeline:_\n",
    "\n",
    "- Tokenizer\n",
    "- Lemmatizer\n",
    "- POS-tagger\n",
    "- NER\n",
    "\n",
    "Sin embargo, por los resultados obtenidos en pruebas concluimos que el NER de Spacy necesita ser mejorado para poder usarlo con confianza en este proceso de extracción de información.\n",
    "\n",
    "En la siguiente celda se extraen los lemmas, el POS-tagging y el dependency parsing de la columna `CONTEX_LECC`, junto con una lista de _dependientes_ de la palabra en el árbol del dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "lemma = []\n",
    "shape = []\n",
    "pos = []\n",
    "istop = []\n",
    "dep = []\n",
    "head = []\n",
    "children = []\n",
    "\n",
    "nlp_text=nlp(cont_lecc)\n",
    "\n",
    "for token in nlp_text:\n",
    "    word.append(token.text)\n",
    "    lemma.append(token.lemma_)\n",
    "    shape.append(token.shape_)\n",
    "    pos.append(token.pos_)\n",
    "    istop.append(token.is_stop)\n",
    "    dep.append(token.dep_)\n",
    "    head.append(token.head.text)\n",
    "    children.append([child for child in token.children])\n",
    "\n",
    "    \n",
    "results = pd.DataFrame({'Word':word, 'Lemma':lemma, 'POS':pos, 'DEP':dep, 'head':head,\n",
    "                             'children':children, 'Shape':shape, 'is_stop':istop})\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo, una vez obtenidas las categorías gramaticales (POS) y las dependencias es extraer entidades y sus relaciones.\n",
    "\n",
    "Esto se puede lograr mediante:\n",
    "* Reglas\n",
    "* Modelos de machine learning no supervisados (clustering o neural networks)\n",
    "\n",
    "Para extraer reglas, se analizaron las gráficas de dependency parsing y se sacaron diferentes reglas observadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "\n",
    "for possible_subject in nlp_text:\n",
    "    # si el POS del HEAD de la palabra es VERB y su dependency parsing es nsubj (sujeto nominal)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.dep_ == 'nsubj' :\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # si las ramas son nmod (modificador nominal) y no es espacio\n",
    "             if child.dep_ in ('nmod') and child.pos_ != 'SPACE': \n",
    "                children.append(child)\n",
    "            \n",
    "        if children:\n",
    "            info.append((possible_subject.head.lemma_.lower(),possible_subject.lemma_.lower(),children))\n",
    "result = pd.DataFrame(info, columns = ['Head' , 'Word', 'Children'])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!\n",
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    # Si el POS del HEAD de la palabra es VERB y el POS de la palabra es un sustantivo (PROPN y NOUN) y\n",
    "    # su dependency parsing es sujeto nominal (nsubj)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN') and possible_subject.dep_=='nsubj':\n",
    "        info.append((possible_subject.head,possible_subject,possible_subject.lemma_))\n",
    "        \n",
    "result_subj = pd.DataFrame(info, columns = ['Head','Word', 'Lemma'])\n",
    "result_subj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    # Si el POS del HEAD de la palabra es VERB y el POS de la palabra es un sustantivo (PROPN y NOUN) y\n",
    "    # su dependency parsing es sujeto nominal (nsubj)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN') and possible_subject.dep_=='nsubj':\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # Solo agregar si no es identificado como espacio\n",
    "            if child.pos_ != 'SPACE':\n",
    "                children.append(child)\n",
    "            \n",
    "        if children:\n",
    "            info.append((possible_subject.head.lemma_,possible_subject,possible_subject.lemma_.lower(),children))\n",
    "            \n",
    "result_subj1 = pd.DataFrame(info, columns = ['Head' , 'Word', 'Lemma', 'Children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    if possible_subject.pos_ == 'VERB' and possible_subject.dep_=='nsubj':\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # Solo agregar si no es identificado como espacio\n",
    "            if child.pos_ != 'SPACE':\n",
    "                children.append(child)\n",
    "            \n",
    "        if children:\n",
    "            info.append((possible_subject.head.lemma_,possible_subject,possible_subject.lemma_.lower(),children))\n",
    "            \n",
    "result_subj2 = pd.DataFrame(info, columns = ['Head' , 'Word', 'Lemma', 'Children'])\n",
    "result_subj2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_subj2.Lemma.value_counts().nlargest(10, keep='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import textacy\n",
    "from collections import defaultdict\n",
    "\n",
    "###\n",
    "# Patrón para extraer información de un texto basado en reglas(con expresiones regulares basados en token).\n",
    "###\n",
    "\n",
    "patron = r'<PROPN>+ (<PUNCT|CCONJ> <PUNCT|CCONJ>? <PROPN>+)*'\n",
    "param = []\n",
    "i = 0\n",
    "while i < len(data[\"CONTEX_LECC\"]):\n",
    "    lists_ = []\n",
    "    sent = nlp(data[\"CONTEX_LECC\"].iloc[i])\n",
    "    doc = textacy.make_spacy_doc(sent, lang='es_core_news_md')\n",
    "    lists_ = textacy.extract.pos_regex_matches(doc, patron)\n",
    "    for item in lists_:\n",
    "        if len(item) != 0:\n",
    "            param.append(item.text.lower())\n",
    "    i +=1\n",
    "\n",
    "j=0\n",
    "aux = defaultdict(list)\n",
    "for index, item in enumerate(param):\n",
    "    aux[item].append(index)\n",
    "\n",
    "result = {item: len(indexs) for item, indexs in aux.items() if len(indexs) >= 1}\n",
    "key = list(result.keys())\n",
    "key.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizar los datos del diccionario de entidades\n",
    "key = list(result.keys())\n",
    "print(len(key))\n",
    "for item in key:\n",
    "    i = 0\n",
    "    key_new = []\n",
    "    key_new = key\n",
    "    key_new.remove(item)\n",
    "    while i < len(key) - 1:        \n",
    "#         print(len(key_new))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos la regla de entidades con la regla de relaciones en la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "\n",
    "for possible_subject in nlp_text:\n",
    "    # si el POS del HEAD de la palabra es VERB y su dependency parsing es nsubj (sujeto nominal)\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.dep_ == 'nsubj' :\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # si las ramas son nmod (modificador nominal) y no es espacio\n",
    "            if str(child).lower() in key:\n",
    "                 if child.dep_ in ('nmod') and child.pos_ != 'SPACE':\n",
    "                        info.append((possible_subject.text.lower(),possible_subject.head.lemma_.lower(),child.text.lower()))\n",
    "\n",
    "result_ent = pd.DataFrame(info, columns = ['Word', 'Head', 'Children'])\n",
    "result_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ent[result_ent.Children == 'asesoftware']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for possible_subject in nlp_text:\n",
    "    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN'):\n",
    "        children = []\n",
    "        for child in possible_subject.children:\n",
    "            # Solo agregar si no es identificado como espacio\n",
    "            if str(child).lower() in key:\n",
    "                if child.pos_ != 'SPACE':\n",
    "                    info.append((possible_subject.text.lower(),possible_subject.head.lemma_.lower(),child.text.lower()))\n",
    "                                    \n",
    "result_ent1 = pd.DataFrame(info, columns = ['Word', 'Head', 'Children'])\n",
    "result_ent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ent1[result_ent1.Children == 'proyecto']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
