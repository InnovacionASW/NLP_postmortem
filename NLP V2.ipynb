{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Trabajar con archivos post-mortem\n\nEn **<font color='0066FB'>Asesoftware</font>** existen archivos con la información post-mortem de los proyectos: los documentos de cierre de proyecto. En este documento, en la sección de lecciones aprendidas, se consignan los aprendizajes del proyecto.\n\nLa información de cada lección aprendida, contexto, proyecto, etc. se encuentra en un archivo que representa la base de datos de lecciones aprendidas aprobadas por la organización. El siguiente script obtiene en un dataframe de Pandas la información de contexto y lección aprendida."},{"metadata":{},"cell_type":"markdown","source":"***\n***\n# Análisis Post-Mortem\n\nEl procesamiento de lenguaje natural (o NLP por sus siglas en inglés) es una rama de conocimiento de la Inteligencia Artificial que, esencialmente, pretende conseguir que una máquina comprenda lo que expresa una persona mediante el uso de una lengua natural.\n\nPara el análisis de los documentos post-mortem se van a seguir los siguientes pasos:\n\n1. Separación de palabras ( _tokenize_ )\n2. Limpieza ( _Stem - Lemma_ )\n3. _Part-Of-Speech tagging_\n4. _Dependency Parsing_\n5. Reconocimiento de entidades ( _Named Entity Recognition_ )\n\nEsto para poder llegar a encontrar **relaciones binarias** entre las entidades halladas en el punto 5."},{"metadata":{},"cell_type":"markdown","source":"### Librerías necesarias"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport spacy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download es_core_news_md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importar vocabulario de Spacy, removiendo del pipeline el NER\nnlp = spacy.load('es_core_news_md', disable=['ner'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def concat_text(pdSeries):\n    pdSeries = pdSeries.str.rstrip('.')\n    return pdSeries.str.cat(sep='. ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nombre_archivo = '../input/REPOSITORIO_LECCIONES APRENDIDAS.xlsx'\n\ndata = pd.read_excel(nombre_archivo, encoding='latin-1', keep_default_na= False, na_values=[\"\"])\ndata[\"CONTEX_LECC\"] = [concat_text(i[1]) for i in data[['CONTEXTO', 'LECCIONES APRENDIDAS']].iterrows()]\ndata.CONTEX_LECC = data.CONTEX_LECC.str.replace('\\n', ' ').replace('\\s+', ' ')\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En la columna `CONTEX_LECC` están concatenadas las oraciones de contexto y lecciones, y `cont_lecc` (en la siguiente celda) es el texto que se va a analizar con el pipeline de NLP."},{"metadata":{"trusted":true},"cell_type":"code","source":"cont_lecc = concat_text(data[\"CONTEX_LECC\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"En Spacy, el modelo entrenado en español incluye en su _pipeline:_\n\n- Tokenizer\n- Lemmatizer\n- POS-tagger\n- NER\n\nSin embargo, por los resultados obtenidos en pruebas concluimos que el NER de Spacy necesita ser mejorado para poder usarlo con confianza en este proceso de extracción de información.\n\nEn la siguiente celda se extraen los lemmas, el POS-tagging y el dependency parsing de la columna `CONTEX_LECC`, junto con una lista de _dependientes_ de la palabra en el árbol del dependency parsing"},{"metadata":{"trusted":true},"cell_type":"code","source":"word = []\nlemma = []\nshape = []\npos = []\nistop = []\ndep = []\nhead = []\nchildren = []\n\nnlp_text=nlp(cont_lecc)\n\nfor token in nlp_text:\n    word.append(token.text)\n    lemma.append(token.lemma_)\n    shape.append(token.shape_)\n    pos.append(token.pos_)\n    istop.append(token.is_stop)\n    dep.append(token.dep_)\n    head.append(token.head.text)\n    children.append([child for child in token.children])\n\n    \nresults = pd.DataFrame({'Word':word, 'Lemma':lemma, 'POS':pos, 'DEP':dep, 'head':head,\n                             'children':children, 'Shape':shape, 'is_stop':istop})\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El objetivo, una vez obtenidas las categorías gramaticales (POS) y las dependencias es extraer entidades y sus relaciones.\n\nEsto se puede lograr mediante:\n* Reglas\n* Modelos de machine learning no supervisados (clustering o neural networks)\n\nPara extraer reglas, se analizaron las gráficas de dependency parsing y se sacaron diferentes reglas observadas."},{"metadata":{"trusted":true},"cell_type":"code","source":"info = []\n\nfor possible_subject in nlp_text:\n    # si el POS del HEAD de la palabra es VERB y su dependency parsing es nsubj (sujeto nominal)\n    if possible_subject.head.pos_ == 'VERB' and possible_subject.dep_ == 'nsubj' :\n        children = []\n        for child in possible_subject.children:\n            # si las ramas son nmod (modificador nominal) y no es espacio\n             if child.dep_ in ('nmod') and child.pos_ != 'SPACE': \n                children.append(child)\n            \n        if children:\n            info.append((possible_subject.head.lemma_.lower(),possible_subject.lemma_.lower(),children))\nresult = pd.DataFrame(info, columns = ['Head' , 'Word', 'Children'])\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# !!!\ninfo = []\nfor possible_subject in nlp_text:\n    # Si el POS del HEAD de la palabra es VERB y el POS de la palabra es un sustantivo (PROPN y NOUN) y\n    # su dependency parsing es sujeto nominal (nsubj)\n    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN') and possible_subject.dep_=='nsubj':\n        info.append((possible_subject.head,possible_subject,possible_subject.lemma_))\n        \nresult_subj = pd.DataFrame(info, columns = ['Head','Word', 'Lemma'])\nresult_subj.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info = []\nfor possible_subject in nlp_text:\n    # Si el POS del HEAD de la palabra es VERB y el POS de la palabra es un sustantivo (PROPN y NOUN) y\n    # su dependency parsing es sujeto nominal (nsubj)\n    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN') and possible_subject.dep_=='nsubj':\n        children = []\n        for child in possible_subject.children:\n            # Solo agregar si no es identificado como espacio\n            if child.pos_ != 'SPACE':\n                children.append(child)\n            \n        if children:\n            info.append((possible_subject.head.lemma_,possible_subject,possible_subject.lemma_.lower(),children))\n            \nresult_subj1 = pd.DataFrame(info, columns = ['Head' , 'Word', 'Lemma', 'Children'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info = []\nfor possible_subject in nlp_text:\n    if possible_subject.pos_ == 'VERB' and possible_subject.dep_=='nsubj':\n        children = []\n        for child in possible_subject.children:\n            # Solo agregar si no es identificado como espacio\n            if child.pos_ != 'SPACE':\n                children.append(child)\n            \n        if children:\n            info.append((possible_subject.head.lemma_,possible_subject,possible_subject.lemma_.lower(),children))\n            \nresult_subj2 = pd.DataFrame(info, columns = ['Head' , 'Word', 'Lemma', 'Children'])\nresult_subj2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(result_subj2.Lemma.value_counts().nlargest(10, keep='all'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import unicode_literals\nimport textacy\nfrom collections import defaultdict\n\n###\n# Patrón para extraer información de un texto basado en reglas(con expresiones regulares basados en token).\n###\n\npatron = r'<PROPN>+ (<PUNCT|CCONJ> <PUNCT|CCONJ>? <PROPN>+)*'\nparam = []\ni = 0\nwhile i < len(data[\"CONTEX_LECC\"]):\n    lists_ = []\n    sent = nlp(data[\"CONTEX_LECC\"].iloc[i])\n    doc = textacy.make_spacy_doc(sent, lang='es_core_news_md')\n    lists_ = textacy.extract.pos_regex_matches(doc, patron)\n    for item in lists_:\n        if len(item) != 0:\n            param.append(item.text.lower())\n    i +=1\n\nj=0\naux = defaultdict(list)\nfor index, item in enumerate(param):\n    aux[item].append(index)\n\nresult = {item: len(indexs) for item, indexs in aux.items() if len(indexs) >= 1}\nkey = list(result.keys())\nkey.sort()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizar los datos del diccionario de entidades\nkey = list(result.keys())\nprint(len(key))\nfor item in key:\n    i = 0\n    key_new = []\n    key_new = key\n    key_new.remove(item)\n    while i < len(key) - 1:        \n#         print(len(key_new))\n        i += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combinamos la regla de entidades con la regla de relaciones en la siguiente celda"},{"metadata":{"trusted":true},"cell_type":"code","source":"info = []\n\nfor possible_subject in nlp_text:\n    # si el POS del HEAD de la palabra es VERB y su dependency parsing es nsubj (sujeto nominal)\n    if possible_subject.head.pos_ == 'VERB' and possible_subject.dep_ == 'nsubj' :\n        children = []\n        for child in possible_subject.children:\n            # si las ramas son nmod (modificador nominal) y no es espacio\n            if str(child).lower() in key:\n                 if child.dep_ in ('nmod') and child.pos_ != 'SPACE':\n                        info.append((possible_subject.text.lower(),possible_subject.head.lemma_.lower(),child.text.lower()))\n\nresult_ent = pd.DataFrame(info, columns = ['Word', 'Head', 'Children'])\nresult_ent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_ent[result_ent.Children == 'asesoftware']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info = []\nfor possible_subject in nlp_text:\n    if possible_subject.head.pos_ == 'VERB' and possible_subject.pos_ in ('PROPN','NOUN'):\n        children = []\n        for child in possible_subject.children:\n            # Solo agregar si no es identificado como espacio\n            if str(child).lower() in key:\n                if child.pos_ != 'SPACE':\n                    info.append((possible_subject.text.lower(),possible_subject.head.lemma_.lower(),child.text.lower()))\n                                    \nresult_ent1 = pd.DataFrame(info, columns = ['Word', 'Head', 'Children'])\nresult_ent1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_ent1[result_ent1.Children == 'proyecto']","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}