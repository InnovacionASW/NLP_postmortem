{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Asesoftware_logo.png\" width=\"200\" height=\"100\">\n",
    "<center>\n",
    "    <h3>MODELO WORD2VEC (WORD EMBEDDINGS) EN DOCUMENTOS POSTMORTEM</h3>\n",
    "    <h3>Autor: <i>Álvaro Valbuena</i></h3>\n",
    "    <h4><i>avalbuena@asesoftware.com</i></h4>\n",
    "    <h3>Área de Innovación</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Representaciones Vectoriales con Word2Vec__\n",
    "\n",
    "_Word2Vec_ es una aproximación que nos ayuda a crear vectores similares para palabras similares. Las palabras que estan relacionadas son mapeadas a puntos que estan cercanos entre si en un espacio dimensional. Los modelos Word2Vec tiene las siguientes ventajas:\n",
    "\n",
    "- Wor2Vec se construye sobre el hecho de que las palabras que comparten contextos también comparten significados semánticos.\n",
    "\n",
    "- Los modelos Wor2Vec predicen una palabra usando sus vecinos a travez del aprendizaje de vectores densos llamados __embeddings__.\n",
    "\n",
    "- Los modelos Wor2Vec tambien son eficientes computacionalmente.\n",
    "\n",
    "- Wor2Vec son modelos no supervizados que aprenden de embeddings de texto bruto.\n",
    "\n",
    "- Son dos los modelos Wor2Vec: __CBOW__ (Cuando la palabra target es predicha usando las palabras contexto) y __Skip-gramv__ (Cuando las palabras contexto son predichas usando la palabra target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec es una red neuronal de 3 capas (Capa de entrada, capa oculta y capa de salida). La capa intermedia (capa oculta) contruye una representación latente para que las palabras de entrada se transformen en la representación del vector de salida.\n",
    "\n",
    "En la representación vectorial de las palabras de Word2Vec se pueden encontrar relaciones matemáticas interesantes como\n",
    "\n",
    "<center>$king - man = queen - woman$</center>\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "Tenemos las siguientes dos frases:\n",
    "\n",
    "- I like watching a movie.\n",
    "- I enjoy watching a movie.\n",
    "\n",
    "Siguiendo el modelo CBOW y tomando las palabras contexto como entrada y tratamos de predecir la palabra target, entonces la salida sería como sigue:\n",
    "\n",
    "![cbow_01](img/cbow_01.jpg)\n",
    "\n",
    "Una forma vectorizada de la entrada y salida luciría algo así:\n",
    "\n",
    "![cbow_02](img/cbow_02.png)\n",
    "\n",
    "En la red neuronal para nuestro ejemplo , habrían 3 neuronas en la capa oculta y en la capa de salida habrían 5 neuronas con funciones _softmax_ de modo que nos darán las probabilidades de las palabras.\n",
    "\n",
    "![NN](img/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <CENTER>CODIGO EN PYTHON DE WORD2VEC SOBRE LOS DOCUMENTOS POSTMORTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías necesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descargar paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c anaconda gensim -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge multiprocess -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge spacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos ayudaremos de spacy para obtener una lista de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas funciones necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta función concatena una serie de pandas\n",
    "def concat_text(pdSeries):\n",
    "    pdSeries = pdSeries.str.rstrip('.')\n",
    "    return pdSeries.str.cat(sep='. ')\n",
    "\n",
    "\n",
    "#Esta función lematiza y elimina las stopwords\n",
    "def cleaning(doc):    \n",
    "    txt = [token.text for token in doc if not token.is_stop]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "    \n",
    "\n",
    "#Esta función filtra stopwords de una lista\n",
    "def filter_stopwords(answ, stopwords):\n",
    "    ans_flt = [token for token in answ if not token[0] in stopwords]\n",
    "    return ans_flt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga y visualización de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data/REPOSITORIO_LECCIONES APRENDIDAS.xlsx\", sheet_name=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, no hay vacíos en las columnas \"CONTEXTO\" y \"LECCIONES APRENDIDAS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['CONTEXTO','LECCIONES APRENDIDAS']].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tomaremos solo el contexto y las lecciones aprendidas. Esto es debido a que nos interesan las lecciones aprendidas pero estas ocurren bajo un contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['CONTEXTO','LECCIONES APRENDIDAS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora crearemos una nueva columna en el dataframe que se llamará __CONTEX_LECC__ la cual tendrá la concatenación del contexto con la lección aprendida. También eliminaremos los saltos de línea y reemplazaremos los multiples espacios por uno solo. Finalmente, habrá un punto seguido entre un contexto y una lección aprendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CONTEX_LECC\"] = [concat_text(i[1]) for i in data[['CONTEXTO', 'LECCIONES APRENDIDAS']].iterrows()]\n",
    "data.CONTEX_LECC = data.CONTEX_LECC.str.replace('\\n', ' ').replace('\\s+', ' ')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estandarización y tokenización del texto. Nuestra estandarización será transformar las letras mayúsculas a minúsculas y la eliminación de la puntuación. El resultado se verá en la columna __CLEANED_CONTEX_LECC__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CLEANED_CONTEX_LECC'] = data['CONTEX_LECC'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s+', ' ')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, necesitaremos hacer dos pruebas, una en la que entrenaremos un modelo Word2Vec (w2v) con las stopwords y otra en la que entrenaremos otro modelo sin las stopwords. Por lo que ahora tomaremos la columna __CLEANED_CONTEX_LECC__ y eliminares las stopwords. El resuldato se podrá ver en la nueva columna llamada __WITHOUT_STOPW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = [cleaning(doc) for doc in nlp.pipe(data.CLEANED_CONTEX_LECC, n_threads=-1)]\n",
    "brief_cleaning = data['CLEANED_CONTEX_LECC']\n",
    "#txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "data['WITHOUT_STOPW'] = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(workers=1, seed=123)\n",
    "w2v_model_sw = Word2Vec(workers=1, seed=123)\n",
    "\n",
    "\n",
    "sentences_1 = data.CLEANED_CONTEX_LECC.str.split()\n",
    "sentences_2 = data.WITHOUT_STOPW.str.split()\n",
    "sentences_2 = sentences_2.dropna()\n",
    "\n",
    "#Entrenamiento del modelo con stopwords\n",
    "w2v_model.build_vocab(sentences_1, progress_per=10000)\n",
    "w2v_model.train(sentences_1, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "#Entrenamiento del modelo sin stopwords\n",
    "w2v_model_sw.build_vocab(sentences_2, progress_per=10000)\n",
    "w2v_model_sw.train(sentences_2, total_examples=w2v_model_sw.corpus_count, epochs=30, report_delay=1)\n",
    "w2v_model_sw.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"El modelo con stopwords tiene %d palabras en su vocabulario\" % len(w2v_model.wv.vocab.keys()))\n",
    "print(\"El modelo sin stopwords tiene %d palabras en su vocabulario\" % len(w2v_model_sw.wv.vocab.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miremos las 15 palabras más cercanas (similares) a _asesoftware_ para cada modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo con stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['asesoftware'], topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo sin stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_sw.wv.most_similar('asesoftware', topn=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los resultados podemos descartar el modelo sin las stopwords ya que muestra que todas las palabras estas muy cercanas entre si (0.99 mínimo). Los resultados que tienen más congruencia son los del modelo entrenado con stopwords. Ahora, para dar mayor valor a estos resultados, ahora si filtremos las stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['asesoftware'], topn=15)\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "spanish_stopwords = ['y', 'e', 'a', 'o', 'u', 'tenia'] #Agregaremos esta lista a las stopwords de spacy\n",
    "\n",
    "for word in spanish_stopwords:\n",
    "    stopwords.add(word)\n",
    "\n",
    "filter_stopwords(w2v_model.wv.most_similar('asesoftware', topn=20), stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
